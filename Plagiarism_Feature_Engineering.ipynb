{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffcfdb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import helper_Arabic\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1e60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a csv file and return a transformed dataframe\n",
    "def numerical_dataframe(csv_file):    \n",
    "    df = pd.read_csv(csv_file, index_col=0)\n",
    "    \n",
    "    df['Class'] = df['plagiarism_types'].map({'no-plagiarism':0,\n",
    "                                      'artificial-obfuscation':1,\n",
    "                                      'no-obfuscation':1,\n",
    "                                      'simulated-obfuscation':1})\n",
    "    \n",
    "    df['category'] = df['plagiarism_types']\n",
    "    \n",
    "    df['category'] = df['category'].map({'no-plagiarism':0,\n",
    "                                      'artificial-obfuscation':1,\n",
    "                                      'no-obfuscation':2,\n",
    "                                      'simulated-obfuscation':3})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e10e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ngram containment for one answer file/source file pair in a df\n",
    "def calculate_containment(n, suspicious_text, source_text):    \n",
    "    #count the n-grams\n",
    "    counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "    ngrams_array = counts.fit_transform([suspicious_text, source_text]).toarray()\n",
    "    \n",
    "    #containment calculation\n",
    "    containment = (np.minimum(ngrams_array[0],ngrams_array[1]).sum())/(ngrams_array[0].sum())\n",
    "    \n",
    "    return containment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b64fccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the normalized LCS given an suspicious text and a source text\n",
    "def lcs_norm_word(suspicious_text, source_text):\n",
    "    '''Computes the longest common subsequence of words in two texts; returns a normalized value.\n",
    "       :param suspicious_text: The pre-processed text for an answer text\n",
    "       :param source_text: The pre-processed text for an answer's associated source text\n",
    "       :return: A normalized LCS value'''\n",
    "    \n",
    "    #split the strings into words using split() and whitespace as a separator\n",
    "    suspicious_words = suspicious_text.split()\n",
    "    source_words = source_text.split()\n",
    "    \n",
    "    #getting the word counts\n",
    "    suspicious_w_counts = len(suspicious_words)\n",
    "    source_w_counts = len(source_words)\n",
    "    \n",
    "    #instantiating a matrix adding extra row and column of zeros\n",
    "    lcs_matrix = np.zeros((source_w_counts + 1, suspicious_w_counts + 1), dtype=int)\n",
    "    \n",
    "    #fillling up the matrix based on matches\n",
    "    for s, s_word in enumerate(source_words, 1):\n",
    "        for a, a_word in enumerate(suspicious_words, 1):\n",
    "            if s_word == a_word:\n",
    "                lcs_matrix[s][a] = lcs_matrix[s-1][a-1] + 1\n",
    "            else:\n",
    "                lcs_matrix[s][a] = max(lcs_matrix[s-1][a], lcs_matrix[s][a-1])\n",
    "    \n",
    "    lcs = lcs_matrix[source_w_counts][suspicious_w_counts]        \n",
    "        \n",
    "    return lcs/suspicious_w_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb1c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_text(text):\n",
    "    str_text = str(text)\n",
    "    len_text = len(str_text)\n",
    "    \n",
    "    return len_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "541fdecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5187c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_preprocess(df):\n",
    "    \n",
    "    col_name = 'suspicious_text'\n",
    "    df = helper_Arabic.complete_preprocess(df, col_name)\n",
    "\n",
    "    col_name = 'source_text'\n",
    "    df = helper_Arabic.complete_preprocess(df, col_name)\n",
    "    \n",
    "    df['len_suspicious_text'] = df['suspicious_text'].apply(lambda x: len_text(x))\n",
    "    df['len_source_text'] = df['source_text'].apply(lambda x: len_text(x))\n",
    "    df['num_words_suspicious_text'] = df['suspicious_text'].apply(lambda x: num_words(x))\n",
    "    df['num_words_source_text'] = df['source_text'].apply(lambda x: num_words(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56838390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_task(df):\n",
    "    \n",
    "    df_temp = df.copy()\n",
    "    df_temp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    n_grams = range(1,8)\n",
    "\n",
    "    containment_values = []\n",
    "    lcs_values = []\n",
    "    col_names = []\n",
    "\n",
    "    for n_gram in n_grams:\n",
    "        col_name = 'c_' + str(n_gram)\n",
    "        col_names.append(col_name)\n",
    "\n",
    "    col_names.append('lcs_value')\n",
    "\n",
    "    df_features = pd.DataFrame(columns = col_names)\n",
    "\n",
    "    for row_i in df_temp.index:\n",
    "        suspicious_text = df_temp.iloc[row_i]['suspicious_text']\n",
    "        source_text = df_temp.iloc[row_i]['source_text']\n",
    "            \n",
    "        features_values = []\n",
    "\n",
    "        for n_gram in n_grams:\n",
    "            containment_value = calculate_containment(n_gram, suspicious_text, source_text)\n",
    "            features_values.append(containment_value)\n",
    "\n",
    "        lcs_value = lcs_norm_word(suspicious_text, source_text)\n",
    "        features_values.append(lcs_value)\n",
    "        df_x = pd.DataFrame([features_values], columns = col_names)\n",
    "\n",
    "        df_features = df_features.append(df_x, ignore_index = True)\n",
    "        \n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44843da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(x, y, file_name, file_dir):\n",
    "    \n",
    "    # in this concatenation the first column will be my labels, the remaining columns are features\n",
    "    pd.concat([pd.DataFrame(y), pd.DataFrame(x)], axis=1).to_csv(os.path.join(file_dir, file_name), header=False, index=False)\n",
    "\n",
    "    print('file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14a9bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "# train dataset\n",
    "csv_file_train = 'data/train-pair-sentences.csv'\n",
    "\n",
    "df_transformed_train = numerical_dataframe(csv_file_train)\n",
    "df_transformed_train.drop(columns='obfuscation_types', axis=1, inplace=True)\n",
    "\n",
    "df_transformed_train = df_preprocess(df_transformed_train)\n",
    "\n",
    "df_features_train = perform_task(df_transformed_train)\n",
    "selected_features = ['c_1', 'c_2', 'c_7', 'lcs_value']\n",
    "\n",
    "train_x = df_features_train[selected_features].values\n",
    "train_y = df_transformed_train['Class'].values\n",
    "\n",
    "file_name_train = 'train.csv'\n",
    "file_dir_train = 'data/'\n",
    "\n",
    "make_csv(train_x, train_y, file_name_train, file_dir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f32cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "train_x = df_features_train.values\n",
    "train_y = df_transformed_train['Class'].values\n",
    "\n",
    "file_name_train = 'train_all.csv'\n",
    "file_dir_train = 'data/'\n",
    "\n",
    "make_csv(train_x, train_y, file_name_train, file_dir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68cd0e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['c_1', 'c_2', 'c_7', 'lcs_value']\n",
    "\n",
    "train_x = df_features_train[selected_features].values\n",
    "train_y = df_transformed_train['category'].values\n",
    "\n",
    "file_name_train = 'train_multi_class.csv'\n",
    "file_dir_train = 'data/'\n",
    "\n",
    "make_csv(train_x, train_y, file_name_train, file_dir_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0469778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time = 11:45:23\n",
      "file created\n",
      "\n",
      "Current Time = 11:52:18\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "# test dataset\n",
    "csv_file_test = 'data/test-pair-sentences.csv'\n",
    "\n",
    "df_transformed_test = numerical_dataframe(csv_file_test)\n",
    "df_transformed_test.drop(columns='obfuscation_types', axis=1, inplace=True)\n",
    "\n",
    "df_transformed_test = df_preprocess(df_transformed_test)\n",
    "\n",
    "df_features_test = perform_task(df_transformed_test)\n",
    "selected_features = ['c_1', 'c_2', 'c_7', 'lcs_value']\n",
    "\n",
    "test_x = df_features_test[selected_features].values\n",
    "test_y = df_transformed_test['Class'].values\n",
    "\n",
    "file_name_test = 'test.csv'\n",
    "file_dir_test = 'data/'\n",
    "\n",
    "make_csv(test_x, test_y, file_name_test, file_dir_test)\n",
    "print()\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "301535cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "test_x = df_features_test.values\n",
    "test_y = df_transformed_test['Class'].values\n",
    "\n",
    "file_name_test_test = 'test_all.csv'\n",
    "file_dir_test = 'data/'\n",
    "\n",
    "make_csv(test_x, test_y, file_name_test, file_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b29c534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file created\n"
     ]
    }
   ],
   "source": [
    "selected_features = ['c_1', 'c_2', 'c_7', 'lcs_value']\n",
    "\n",
    "test_x = df_features_test[selected_features].values\n",
    "test_y = df_transformed_test['category'].values\n",
    "\n",
    "file_name_test = '_test.csv'\n",
    "file_dir_test = 'data/'\n",
    "\n",
    "make_csv(test_x, test_y, file_name_test, file_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3650a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5d8100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721cfb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
